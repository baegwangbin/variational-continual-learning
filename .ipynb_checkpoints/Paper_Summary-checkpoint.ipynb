{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Paper Summary \n",
    "\n",
    "This is a summary of the paper **Variational Continual Learning** written by **Cuong V. Nguyen**, **Yingzhen Li**, **Thang D. Bui** and **Richard E. Turner**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Introduction\n",
    "\n",
    "* **Continual Learning**\n",
    "\n",
    ">* Data continuously arrive in a possibly non i.i.d. way\n",
    ">* Tasks may change over time (e.g. new classes may be discovered)\n",
    ">* Entirely new tasks can emerge ([Schlimmer & Fisher 1986](references/Schlimmer&Fisher_1986.pdf); [Sutton & Whitehead, 1993](references/Sutton&Whitehead_1993.pdf); [Ring, 1997](references/Ring_1997.pdf))\n",
    "\n",
    "* **Challenge for Continual Learning**\n",
    "\n",
    ">* Balance between **adapting to new data** vs. **retaining existing knowledge**\n",
    ">  * Too much plasticity $\\rightarrow$ **catastrophic forgetting** ([McCloskey & Cohen, 1989](https://www.sciencedirect.com/science/article/pii/S0079742108605368); [Ratcliff, 1990](references/Ratcliff_1990.pdf); [Goodfellow et al., 2014a](references/Goodfellow_2014a.pdf))\n",
    ">  * Too much stability $\\rightarrow$ inability to adapt\n",
    ">* **Approach 1:** train individual models on each task $\\rightarrow$ train to combine them\n",
    ">  * ([Lee et al., 2017](references/Lee_2017.pdf))\n",
    ">* **Approach 2:** maintain a single model and use a single type of regularized training that prevents drastic changes in the influential parameters, but allow other parameters to change more freely\n",
    ">  * ([Li & Hoiem, 2016](references/Li&Hoiem_2016.pdf); [Kirkpatrick et al., 2017](references/Kirkpatrick_2017.pdf); [Zenke et al., 2017](references/Zenke_2017.pdf))\n",
    "\n",
    "* **Variational Continual Learning**\n",
    "\n",
    ">* Merge **online VI** ([Ghahramani & Attias, 2000](references/Ghahramani&Attias_2000.pdf); [Sato, 2001](references/Sato_2001.pdf); [Broderick et al., 2013](references/Broderick_2013.pdf))\n",
    ">* with **Monte Carlo VI for NN** ([Blundell et al., 2015](references/Blundell_2015.pdf))\n",
    ">* and include a **small episodic memory** ([Bachem et al., 2015](references/Bachem_2015.pdf); [Huggins et al., 2016](references/Huggins_2016.pdf))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Continual Learning by Approximate Bayesian Inference\n",
    "\n",
    "* **Online updating, derived from Bayes' rule**\n",
    "\n",
    ">$$p(\\boldsymbol{\\theta}|\\mathcal{D}_{1:T}) \\propto p(\\boldsymbol{\\theta}) \\prod^T_{t=1} \\prod^{N_t}_{n_t=1} p(y_t^{(n_t)}|\\boldsymbol{\\theta},x_t^{(n_t)}) = p(\\boldsymbol{\\theta}) \\prod^T_{t=1} p(\\mathcal{D}_t|\\boldsymbol{\\theta}) \\propto p(\\boldsymbol{\\theta}|\\mathcal{D}_{1:T-1}) p(\\mathcal{D}_T|\\boldsymbol{\\theta})$$\n",
    "\n",
    ">* Posterior after $T$th dataset $\\propto$ Posterior after $(T-1)$th dataset $\\times$ Likelihood of the $T$th dataset\n",
    "\n",
    "* **Projection Operation: approximation for intractable posterior** (recursive)\n",
    "\n",
    ">\\begin{align}\n",
    "p(\\boldsymbol{\\theta}|\\mathcal{D}_1) \\approx q_1(\\boldsymbol{\\theta}) &= \\text{proj}(p(\\boldsymbol{\\theta})p(\\mathcal{D}_1|\\boldsymbol{\\theta})) \\\\\n",
    "p(\\boldsymbol{\\theta}|\\mathcal{D}_{1:T}) \\approx q_T(\\boldsymbol{\\theta}) &= \\text{proj}(q_{T-1}(\\boldsymbol{\\theta})p(\\mathcal{D}_T|\\boldsymbol{\\theta}))\n",
    "\\end{align}\n",
    "\n",
    ">|Projection Operation|Inference Method|References|\n",
    "|-|-|-|\n",
    "|Laplace's approximation    |Laplace propagation      |[Smola et al., 2004](references/Smola_2004.pdf)|\n",
    "|Variational KL minimization|Online VI|[Ghahramani & Attias, 2000](references/Ghahramani&Attias_2000.pdf); [Sato, 2001](references/Sato_2001.pdf)|\n",
    "|Moment matching            |Assumed density filtering|[Maybeck, 1982](references/Maybeck_1982.pdf)|\n",
    "|Importance sampling        |Sequential Monte Carlo   |[Liu & Chen, 1998](references/Liu&Chen_1998.pdf)|\n",
    "\n",
    ">* This paper will use **Online VI** as it outperforms other methods for complex models in the static setting ([Bui et al., 2016](references/Bui_2016.pdf))\n",
    ">* <font color='red'>** Q. Try building VCL with different projection operation? **</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.1. VCL and Episodic Memory Enhancement\n",
    "\n",
    "* **Projection Operation: KL Divergence Minimization**\n",
    "\n",
    ">$$q_t(\\boldsymbol{\\theta}) = \\underset{q \\in \\mathcal{Q}}{\\text{argmin}} \\text{KL} \n",
    "\\left( q(\\boldsymbol{\\theta}) || \\frac{1}{Z_t} q_{t-1}(\\boldsymbol{\\theta}) p(\\mathcal{D}_t|\\boldsymbol{\\theta}) \\right)$$\n",
    "\n",
    ">* $q_0(\\boldsymbol{\\theta}) = p(\\boldsymbol{\\theta})$\n",
    ">* $Z_t$: normalizing constant (not required when computing the optimum)\n",
    ">* VCL becomes Bayesian inference if $p(\\boldsymbol{\\theta}|\\mathcal{D}_{1:t}) \\in \\mathcal{Q} \\;\\forall\\; t$\n",
    "\n",
    "* **Potential Problems**\n",
    "\n",
    ">* Errors from repeated approximation $\\rightarrow$ forget old tasks\n",
    ">* Minimization at each step is also approximate $\\rightarrow$ information loss\n",
    "\n",
    "* **Solution: Coreset**\n",
    "\n",
    ">* **Coreset:** small representative set of data from previously observed tasks\n",
    ">  * Analogous to **episodic memory** ([Lopez-Paz & Ranzato, 2017](references/Lopez-Paz&Ranzato_2017.pdf))\n",
    ">* **Coreset VCL:** equivalent to a message-passing implementation of VI in which the coreset data point updates are scheduled after updating the other data\n",
    "\n",
    ">* <font color='red'>** Q. Implement coreset VCL using message-passing? **</font>\n",
    "\n",
    ">* $C_t$: updated using $C_{t-1}$ and selected data points from $\\mathcal{D}_t$ (e.g. random selection, K-center algorithm, ...)\n",
    ">  * K-center algorithm: return K data points that are spread throughout the input space ([Gonzalez, 1985](references/Gonzalez_1985.pdf))\n",
    ">  * <font color='red'>** Q. Finding optimal way of selecting the coreset? **</font>\n",
    "\n",
    "* **Variational Recursion**\n",
    "\n",
    ">$$p(\\boldsymbol{\\theta}|\\mathcal{D}_{1:t}) \\propto p(\\boldsymbol{\\theta}|\\mathcal{D}_{1:t} \\setminus C_t) p(C_t|\\boldsymbol{\\theta}) \\approx \\tilde{q}_t (\\boldsymbol{\\theta}) p(C_t|\\boldsymbol{\\theta})$$\n",
    "\n",
    ">$$p(\\boldsymbol{\\theta}|\\mathcal{D}_{1:t} \\setminus C_t) = p(\\boldsymbol{\\theta}|\\mathcal{D}_{1:t-1} \\setminus C_{t-1}) p(C_{t-1} \\setminus C_t | \\boldsymbol{\\theta}) p(\\mathcal{D}_t \\setminus C_t | \\boldsymbol{\\theta}) \\approx \\tilde{q}_{t-1}(\\boldsymbol{\\theta}) p(\\mathcal{D}_t \\cup C_{t-1} \\setminus C_t | \\boldsymbol{\\theta})$$\n",
    "\n",
    "* **Algorithm**\n",
    "\n",
    ">* **Step 1:** Observe $\\mathcal{D}_t$\n",
    ">* **Step 2:** Update $C_t$ using $C_{t-1}$ and $\\mathcal{D}_t$\n",
    ">* **Step 3:** Update $\\tilde{q}_t$ (used for **propagation**)\n",
    "\n",
    ">\\begin{align}\n",
    "\\tilde{q}_t(\\boldsymbol{\\theta}) &= \\text{proj} \\left( \\tilde{q}_{t-1}(\\boldsymbol{\\theta}) p(\\mathcal{D}_t \\cup C_{t-1} \\setminus C_t | \\boldsymbol{\\theta}) \\right) \\\\\n",
    "&= \\underset{q \\in \\mathcal{Q}}{\\text{argmin}} \\; \\text{KL} \n",
    "\\left( q(\\boldsymbol{\\theta})  \\;\\big|\\big|\\; \\frac{1}{\\tilde{Z}} \\tilde{q}_{t-1}(\\boldsymbol{\\theta}) p(\\mathcal{D}_t \\cup C_{t-1} \\setminus C_t |\\boldsymbol{\\theta}) \\right)\n",
    "\\end{align}\n",
    "\n",
    ">* **Step 4:** Update $q_t$ (used for **prediction**)\n",
    "\n",
    ">\\begin{align}\n",
    "q_t(\\boldsymbol{\\theta}) &= \\text{proj} \\left( \\tilde{q}_{t}(\\boldsymbol{\\theta}) p(C_t | \\boldsymbol{\\theta}) \\right) \\\\\n",
    "&= \\underset{q \\in \\mathcal{Q}}{\\text{argmin}} \\; \\text{KL} \n",
    "\\left( q(\\boldsymbol{\\theta})  \\;\\big|\\big|\\; \\frac{1}{Z} \\tilde{q}_t (\\boldsymbol{\\theta}) p(C_t |\\boldsymbol{\\theta}) \\right)\n",
    "\\end{align}\n",
    "\n",
    ">* **Step 5:** Perform prediction\n",
    "\n",
    ">$$p(y^*|\\boldsymbol{x}^*, \\mathcal{D}_{1:t}) = \\int q_t(\\boldsymbol{\\theta}) p(y^*|\\boldsymbol{\\theta},\\boldsymbol{x}^*) d\\boldsymbol{\\theta}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. VCL in Deep Discriminative Models\n",
    "\n",
    "* **Multi-head Networks**\n",
    "\n",
    ">* Standard architecture used for multi-task learning ([Bakker & Heskes, 2003](references/Bakker&Heskes_2003.pdf))\n",
    ">* Share parameters close to the inputs / Separate heads for each output\n",
    ">* **More advanced model structures:**\n",
    ">  * for continual learning ([Rusu et al., 2016](references/Rusu_2016.pdf))\n",
    ">  * for multi-task learning in general ([Swietojanski & Renals, 2014](references/Swietojanski&Renals_2014.pdf); [Rebuffi et al., 2017](references/Rebuffi_2017.pdf))\n",
    ">  * **automatic continual model building:** adding new structure as new tasks are encountered\n",
    ">  * <font color='red'>** Q. Implementing VCL for different model architecture? **</font>\n",
    ">* This paper assumes that the model structure is known *a priori*\n",
    "\n",
    "* **Formulation**\n",
    "\n",
    "><img src = 'images/summary_1.png' width=350>\n",
    "\n",
    ">* Model parameter $\\boldsymbol{\\theta} = \\{ \\boldsymbol{\\theta}^H_{1:T}, \\boldsymbol{\\theta}^S \\} \\in \\mathbb{R}^D$\n",
    ">  * **Shared parameters:** updated constantly \n",
    ">  * **Head parameter:** $q(\\boldsymbol{\\theta}^H_K) = p(\\boldsymbol{\\theta}^H_K)$ at the beginning, updated incrementally as each task emerges\n",
    "\n",
    ">* For simplicity, use **Gaussian mean-field approximate posterior** $q_t(\\boldsymbol{\\theta}) = \\prod^D_{d=1} \\mathcal{N} (\\theta_{t,d} ; \\mu_{t,d}, \\sigma^2_{t,d})$\n",
    "\n",
    "* **Network Training**\n",
    "\n",
    ">* Maximize the negative online variational free energy or the variational lower bound to the online marginal likelihood $\\mathcal{L}^t_{VCL}$ with respect to the variational parameters $\\{\\mu_{t,d},\\sigma_{t,d}\\}^D_{d=1}$\n",
    "\n",
    ">$$\\mathcal{L}^t_{VCL} (q_t(\\boldsymbol{\\theta})) = \\sum^{N_t}_{n=1} \\mathbb{E}_{\\boldsymbol{\\theta} \\sim q_t(\\boldsymbol{\\theta})} \\left[ \\log p(y_t^{(n)}|\\boldsymbol{\\theta},\\mathbf{x}^{(n)}_t) \\right] - \\text{KL} (q_t(\\boldsymbol{\\theta})||q_{t-1}(\\boldsymbol{\\theta}))$$\n",
    "\n",
    ">* $\\text{KL} (q_t(\\boldsymbol{\\theta})||q_{t-1}(\\boldsymbol{\\theta}))$: tractable / set $q_0(\\boldsymbol{\\theta})$ as multivariate Gaussian ([Graves, 2011](references/Graves_2011.pdf); [Blundell et al., 2015](references/Blundell_2015.pdf))\n",
    ">* $\\mathbb{E}_{\\boldsymbol{\\theta} \\sim q_t(\\boldsymbol{\\theta})} [\\cdot]$: intractable $\\rightarrow$ approximate by employing simple Monte Carlo and using the **local reparameterization trick** to compute the gradients ([Salimans & Knowles, 2013](references/Salimans&Knowles_2013.pdf); [Kingma & Welling, 2014](references/Kingma&Welling_2014.pdf); [Kingma et al., 2015](references/Kingma_2015.pdf))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4. VCL in Deep Generative Models\n",
    "\n",
    "* **Deep Generative Models**\n",
    "\n",
    ">* Can generate realistic images, sounds, and video sequences ([Chung et al., 2015](references/Chung_2015.pdf); [Kingma et al., 2016](references/Kingma_2016.pdf); [Vondrick et al., 2016](references/Vondrick_2016.pdf))\n",
    ">* Standard batch learning assumes observations to be i.i.d. and are all available at the same time\n",
    ">* This paper applies VCL framework to **variational auto encoders** ([Kingma & Welling, 2014](references/Kingma&Welling_2014.pdf); [Rezende et al., 2014](references/Rezende_2014.pdf))\n",
    "\n",
    ">* <font color='red'>** Q. Applying VCL for GAN([Goodfellow et al., 2014b](references/Goodfellow_2014b.pdf))? **</font> - Initial attempt to apply continual learning ([Seff et al., 2017](references/Seff_2017.pdf))\n",
    "\n",
    "* **Formulation** - VAE approach (batch learning)\n",
    "\n",
    ">$$p(\\mathbf{x}|\\mathbf{z},\\boldsymbol{\\theta}) p(\\mathbf{z})$$\n",
    "\n",
    ">* $p(\\mathbf{z})$: prior over latent variables / typically Gaussian\n",
    ">* $p(\\mathbf{x}|\\mathbf{z},\\boldsymbol{\\theta})$: defined by DNN, $\\boldsymbol{f_\\theta} (\\mathbf{z})$, where $\\boldsymbol{\\theta}$ collects weight matrices and bias vectors\n",
    ">* **Learning $\\boldsymbol{\\theta}$:** approximate MLE (maximize variational lower bound w.r.t. $\\boldsymbol{\\theta}$ and $\\boldsymbol{\\phi}$)\n",
    "\n",
    ">$$\\mathcal{L}_{\\text{VAE}} (\\boldsymbol{\\theta},\\boldsymbol{\\phi}) = \\sum^N_{n=1} \\mathbb{E}_{q_\\boldsymbol{\\phi}(\\mathbf{z}^{(n)}|\\mathbf{x}^{(n)})}\n",
    "\\left[ \\log \\frac{p(\\mathbf{x}^{(n)}|\\mathbf{z}^{(n)},\\boldsymbol{\\theta})p(\\mathbf{z}^{(n)})} {q_\\boldsymbol{\\phi} (\\mathbf{z}^{(n)}|\\mathbf{x}^{(n)})} \\right]$$\n",
    "\n",
    ">* $\\rightarrow$ **No parameter uncertainty estimates** (used to weight the information learned from old data)\n",
    "\n",
    "* **Formulation** - VCL approach (continual learning)\n",
    "\n",
    ">* Approximate full posterior over parametrs, $q_t(\\boldsymbol{\\theta}) \\approx p(\\boldsymbol{\\theta}|\\mathcal{D}_{1:t})$\n",
    ">* Maximize **full** variational lower bound w.r.t. $q_t$ and $\\phi$\n",
    "\n",
    ">$$\\mathcal{L}^t_{\\text{VAE}} (q_t(\\boldsymbol{\\theta}),\\boldsymbol{\\phi}) = \n",
    "\\mathbb{E}_{q_t(\\boldsymbol{\\theta})}\\left\\{\n",
    "\\sum^{N_t}_{n=1} \\mathbb{E}_{q_\\boldsymbol{\\phi}(\\mathbf{z}_t^{(n)}|\\mathbf{x}_t^{(n)})}\n",
    "\\left[ \\log \\frac{p(\\mathbf{x}_t^{(n)}|\\mathbf{z}_t^{(n)},\\boldsymbol{\\theta})p(\\mathbf{z}_t^{(n)})} {q_\\boldsymbol{\\phi} (\\mathbf{z}_t^{(n)}|\\mathbf{x}_t^{(n)})} \\right] \\right\\}\n",
    "-\\text{KL}(q_t(\\boldsymbol{\\theta})||q_{t-1}(\\boldsymbol{\\theta}))$$\n",
    "\n",
    ">* $\\boldsymbol{\\phi}$: task-specific $\\rightarrow$ likely to be beneficial to share (parts of) these encoder networks\n",
    "\n",
    "* **Model Architecture**\n",
    "\n",
    "><img src = 'images/summary_2.png' width=350>\n",
    "\n",
    ">* Latent variables $\\mathbf{z}$ $\\rightarrow$ Intermediate-level representations $\\mathbf{h}$ $\\rightarrow$ Observations $\\mathbf{x}$\n",
    "\n",
    ">* **Architecture 1:** shared bottom network - suitable when data are composed of a common set of structural primitives (e.g. strokes)\n",
    ">* **Architecture 2:** shared head network - information tend to be entirely encoded in bottom network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Related Work\n",
    "\n",
    "* **Continual Learning for Deep Discriminative Models** (regularized MLE)\n",
    "\n",
    ">$$\\mathcal{L}^t (\\boldsymbol{\\theta}) = \\sum^{N_t}_{n=1} \\log p(y_t^{(n)} | \\boldsymbol{\\theta},\\mathbf{x}^{(n)}_t) - \\frac{1}{2} \\lambda_t (\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_{t-1})^T \\Sigma^{-1}_{t-1} (\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_{t-1})$$\n",
    "\n",
    ">* **ML Estimation** - set $\\lambda_t = 0$\n",
    "\n",
    ">* **MAP Estimation** - assume Gaussian prior $q(\\boldsymbol{\\theta}|\\mathcal{D}_{1:t-1})=\\mathcal{N}(\\boldsymbol{\\theta};\\boldsymbol{\\theta}_{t-1},\\Sigma_{t-1}/\\lambda_t)$\n",
    ">  * $\\Sigma_t=? \\; \\rightarrow \\; \\Sigma_t=I$ and use CV to find $\\lambda_T$ $\\rightarrow$ catastrophic forgetting\n",
    "\n",
    ">* **Laplace Propagation (LP)** ([Smola et al., 2004](references/Smola_2004.pdf)) - recursion for $\\Sigma_t$ using Laplace's approximation\n",
    ">  * Diagonal LP: retain only the diagonal terms of $\\Sigma^{-1}_t$ to avoid computing full Hessian\n",
    "\n",
    ">$$\\Sigma^{-1}_t = \\Phi_t + \\Sigma^{-1}_{t-1} \\;\\;\\;,\\;\\;\\; \\Phi_t = - \\nabla \\nabla_\\boldsymbol{\\theta} \\sum^{N_t}_{n=1} \\log p(y^{(n)}_t | \\boldsymbol{\\theta}, \\mathbf{x}^{(n)}_t) \\big|_{\\boldsymbol{\\theta} = \\boldsymbol{\\theta}_t} \\;\\;\\;,\\;\\;\\; \\lambda_t = 1$$\n",
    "\n",
    ">* **Elastic Weight Consolidation (EWC)** ([Kirkpatrick et al., 2017](references/Kirkpatrick_2017.pdf)) - modified diagonal LP\n",
    ">  * Approximate the average Hessian of the likelihoods using Fisher information \n",
    ">$$$$\n",
    ">$$\\Phi_t \\approx \\text{diag} \\left( \\sum^{N_t}_{n=1} \\left( \\nabla_\\boldsymbol{\\theta} \\log p(y^{(n)}_t|\\boldsymbol{\\theta},\\mathbf{x}^{(n)}_t) \\right)^2 \\;\\Big|_{\\boldsymbol{\\theta}=\\boldsymbol{\\theta}_t}\\right)$$\n",
    ">$$$$\n",
    ">  * Regularization term: introduce hyperparameter, remove prior, regularize intermediate estimates\n",
    "\n",
    ">$$\\frac{1}{2} (\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_{t-1})^T (\\Sigma^{-1}_0 + \\Sigma^{t-1}_{t'=1} \\Phi_{t'}) (\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_{t-1})\n",
    "\\rightarrow\n",
    "\\frac{1}{2} \\sum^{t-1}_{t'=1} \\lambda_{t'} (\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_{t'-1})^T \\Phi_{t'} (\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_{t'-1})$$\n",
    "\n",
    ">* **Synaptic Intelligence (SI)** ([Zenke et al., 2017](references/Zenke_2017.pdf)) - compute $\\Sigma^{-1}_t$ using importance of each parameter to each task\n",
    "\n",
    ">* <font color='red'>** Q. How VCL differs from the above methods **</font>\n",
    "\n",
    "\n",
    "* **Approximate Bayesian Training of NN** (focused on )\n",
    "\n",
    ">|Approach|References|\n",
    "|-|-|\n",
    "|**extended Kalman filtering**|[Singhal & Wu, 1989](references/Singhal&Wu_1989.pdf)|\n",
    "|**Laplace's approximation**|[MacKay, 1992](references/MacKay_1992.pdf)|\n",
    "|**variational inference**|[Hinton & Van Camp, 1993](references/Hinton&VamCamp_1993.pdf); [Barber & Bishop, 1998](references/Barber&Bishop_1998.pdf); [Graves, 2011](references/Graves_2011.pdf); [Blundell et al., 2015](references/Blundell_2015.pdf); [Gal & Ghahramani, 2016](references/Gal&Ghahramani_2016.pdf)|\n",
    "|**sequential Monte Carlo**|[de Freitas et al., 2000](references/deFreitas_2000.pdf)|\n",
    "|**expectation propagation**|[Hernández-Lobato & Adams, 2015](references/HernandezLobato&Adams_2015.pdf)|\n",
    "|**approximate power EP**|[Hernández-Lobato et al., 2016](references/HernandezLobato_2016.pdf)|\n",
    "\n",
    ">* <font color='red'>** Q. Apply continual learning framework to these approaches **</font>\n",
    "\n",
    "* **Continual Learning for Deep Generative Models**\n",
    "\n",
    ">* **Naïve approach:** apply VAE to $\\mathcal{D}_t$ with parameters initialized at $\\boldsymbol{\\theta}_{t-1}$ $\\rightarrow$ catastrophic forgetting\n",
    ">* **Alternative:** add EWC regularization term to VAE objective & approximate marginal likelihood by variational lower bound\n",
    ">  * Similar approximation can be used for **Hessian matrices for LP** and **$\\Sigma^{-1}_t$ for SI** (Importance sampling: [Burda et al., 2016](references/Burda_2016.pdf))\n",
    "\n",
    ">$$\\mathcal{L}^t_{EWC} \n",
    "(\\boldsymbol{\\theta},\\boldsymbol{\\phi}) = \\sum^{N_t}_{n=1} \\mathbb{E}_{q_\\boldsymbol{\\phi}(\\mathbf{z}_t^{(n)}|\\mathbf{x}_t^{(n)})}\n",
    "\\left[ \\log \\frac{p(\\mathbf{x}_t^{(n)}|\\mathbf{z}_t^{(n)},\\boldsymbol{\\theta})p(\\mathbf{z}_t^{(n)})} {q_\\boldsymbol{\\phi} (\\mathbf{z}_t^{(n)}|\\mathbf{x}_t^{(n)})} \\right] \n",
    "- \\frac{1}{2} \\sum^{t-1}_{t'=1} \\lambda_{t'} (\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_{t'-1})^T \\Phi_{t'} (\\boldsymbol{\\theta} - \\boldsymbol{\\theta}_{t'-1})$$\n",
    "\n",
    ">$$\\Phi_t \\approx \\text{diag} \\left( \\sum^{N_t}_{n=1} \\left( \\nabla_\\boldsymbol{\\theta} \n",
    "\\mathbb{E}_{q_{\\boldsymbol{\\phi}}(\\mathbf{z}_t^{(n)}|\\mathbf{x}_t^{(n)})}\n",
    "\\left[ \\log \\frac{p(\\mathbf{x}_t^{(n)}|\\mathbf{z}_t^{(n)},\\boldsymbol{\\theta})p(\\mathbf{z}_t^{(n)})} {q_\\boldsymbol{\\phi} (\\mathbf{z}_t^{(n)}|\\mathbf{x}_t^{(n)})} \\right]\n",
    "\\right)^2 \\;\\Bigg|_{\\boldsymbol{\\theta}=\\boldsymbol{\\theta}_t}\\right)$$\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
